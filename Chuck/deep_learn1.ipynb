{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution CNN for leaning images and processing - using high-level Keras preprocessing utilities and layers to read a directory of images on disk.\n",
    "\n",
    "https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deep_learn1.ipynb - was run for 1 year images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "candlesticks/<br />\n",
    "    buy<br />\n",
    "    sell<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define path for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "data_dir = pathlib.Path('candlesticks')\n",
    "data_test_dir = pathlib.Path('candlesticks_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate total candlestick counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.png')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "image_test_count = len(list(data_test_dir.glob('*/*.png')))\n",
    "print(image_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read a sample buy image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "candlesticks_buy = list(data_dir.glob('buy/*.png'))\n",
    "PIL.Image.open(str(candlesticks[0]))\n",
    "#PIL.Image.open(str(candlesticks[0])).size\n",
    "candlesticks_sell = (list(data_dir.glob('sell/*.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "candlesticks_all = candlesticks_buy + candlesticks_sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candlesticks_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "candlesticks_test_buy = list(data_test_dir.glob('buy/*.png'))\n",
    "candlesticks_test_sell = list(data_test_dir.glob('sell/*.png'))\n",
    "candlesticks_test_all = candlesticks_test_buy + candlesticks_test_sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('candlesticks_test/buy/2021-07-25.png'),\n",
       " WindowsPath('candlesticks_test/sell/2021-07-18.png')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candlesticks_test_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load using tf.keras.preprocessing.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataset\n",
    "\n",
    "##### Define some parameters for the loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 1\n",
    "img_height = 255\n",
    "img_width = 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It's good practice to use a validation split when developing your model. Will use 80% of the images for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 files belonging to 2 classes.\n",
      "Using 43 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width)\n",
    "#  batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 files belonging to 2 classes.\n",
      "Using 10 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width)\n",
    "  #batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You can find the class names in the class_names attribute on these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buy', 'sell']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here are the first 9 images from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for images, labels in train_ds.take(1):\n",
    "#     for i in range(2):\n",
    "#         ax = plt.subplot(3, 3, i + 1)\n",
    "#         plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#         plt.title(class_names[labels[i]])\n",
    "#         plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### we wil train a model using these datasets by passing them to model.fit. You can also manually iterate over the dataset and retrieve batches of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 255, 255, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The image_batch is a tensor of the shape (1, 180, 180, 3). This is a batch of 1 images of shape 180x180x3 (the last dimension referes to color channels RGB). The label_batch is a tensor of the shape (1,), these are the corresponding labels to the 32 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The RGB channel values are in the [0, 255] range. This is not ideal for a neural network; in general you should seek to make your input values small. Here, you will standardize values to be in the [0, 1] range by using the tf.keras.layers.experimental.preprocessing.Rescaling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are two ways to use this layer. You can apply it to the dataset by calling map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.8575005\n"
     ]
    }
   ],
   "source": [
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixels values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Or, you can include the layer inside your model definition to simplify deployment. We will use the second approach here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the dataset for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure to use buffered prefetching, so you can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data.\n",
    "\n",
    ".cache() keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
    "\n",
    ".prefetch() overlaps data preprocessing and model execution while training.\n",
    "\n",
    "Interested readers can learn more about both methods, as well as how to cache data to disk in the data performance guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, you will show how to train a simple model using the datasets you have just prepared. This model has not been tuned in any way - the goal is to show you the mechanics using the datasets you just created. To learn more about image classification, visit this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "#  tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 images belonging to 2 classes.\n",
      "Found 53 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=False)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "            data_dir,\n",
    "            target_size=(128, 128),\n",
    "            batch_size=32,\n",
    "            class_mode='categorical')\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(\n",
    "            data_dir,\n",
    "            target_size=(128, 128),\n",
    "            batch_size=32,\n",
    "            class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "            '/content/drive/My Drive/Colab Notebooks/dataset',\n",
    "            target_size=(128, 128),\n",
    "            batch_size=30,\n",
    "            class_mode='categorical')\n",
    "\n",
    "# test_set = test_datagen.flow_from_directory(\n",
    "#             '../dataset/test_set/',\n",
    "#             target_size=(64, 64),\n",
    "#             batch_size=32,\n",
    "#             class_mode='binary')\n",
    "\n",
    "classifier.fit_generator(\n",
    "            training_set,\n",
    "            steps_per_epoch=40,\n",
    "            epochs=10,\n",
    "            validation_data=training_set,\n",
    "            validation_steps=80)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy', #tf.losses.categorical_crossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22bc4eabb88>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(training_set.next()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6875 - accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 1s 639ms/step - loss: 0.6614 - accuracy: 0.6190\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 1s 679ms/step - loss: 0.5407 - accuracy: 0.7500\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 1s 708ms/step - loss: 0.5155 - accuracy: 0.7188\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5435 - accuracy: 0.6562\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 1s 598ms/step - loss: 0.5815 - accuracy: 0.6667\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 0.3687 - accuracy: 0.9048\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 1s 552ms/step - loss: 0.5378 - accuracy: 0.7619\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 0.3872 - accuracy: 0.8571\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 1s 756ms/step - loss: 0.3908 - accuracy: 0.8438\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 1s 518ms/step - loss: 0.2786 - accuracy: 0.9048\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 0.3146 - accuracy: 0.8571\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 0.1861 - accuracy: 0.9524\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 0.1383 - accuracy: 0.9524\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 0.3838 - accuracy: 0.8571\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5562 - accuracy: 0.7143\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 1s 512ms/step - loss: 0.3819 - accuracy: 0.8095\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 1s 628ms/step - loss: 0.2665 - accuracy: 0.8438\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 1s 591ms/step - loss: 0.1452 - accuracy: 0.9688\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 1s 899ms/step - loss: 0.1961 - accuracy: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22bc50ada08>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "            training_set, \n",
    "#            train_ds,\n",
    "            steps_per_epoch=1,\n",
    "            epochs=20,\n",
    "#            validation_split=.1\n",
    "#            validation_data=validation_set\n",
    "#            validation_data=training_set,\n",
    "#            validation_steps=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-a46cb1f2ba6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#  validation_data=val_ds,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m   \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#  verbose='2'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "  train_ds,\n",
    "#  validation_data=val_ds,\n",
    "  epochs=10,\n",
    "#  verbose='2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.evaluate(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.PrefetchDataset"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the next candlesticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import rescale, resize\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def predict_chart(img):\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    test_image = np.expand_dims(img, axis=0)\n",
    "    pred = model.predict_classes(test_image)[[0]]\n",
    "    \n",
    "    if (pred == 0):\n",
    "        print('Sell', pred)\n",
    "    elif (pred == 1):\n",
    "        print('Buy', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candlesticks_test\\buy\\2021-07-25.png\n",
      "PNG\n",
      "RGBA\n",
      "(640, 480)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1569 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1559 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1552 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1525 predict_step\n        return self(x, training=False)\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:255 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential_1 is incompatible with the layer: expected axis -1 of input shape to have value 3 but received input with shape (None, 480, 640, 4)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-6b7f4b452a95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mimg_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mimg_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     print(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1725\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1727\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1728\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m-> 3022\u001b[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3439\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[0;32m   3440\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[1;32m-> 3441\u001b[1;33m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[0;32m   3361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m     graph_function = self._create_graph_function(\n\u001b[1;32m-> 3363\u001b[1;33m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[0;32m   3364\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3289\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1569 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1559 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1552 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1525 predict_step\n        return self(x, training=False)\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\chakravartiraghavan\\anaconda3\\envs\\deepenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:255 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential_1 is incompatible with the layer: expected axis -1 of input shape to have value 3 but received input with shape (None, 480, 640, 4)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(candlesticks_test_all)):\n",
    "    print(candlesticks_test_all[i])\n",
    "    image = Image.open(candlesticks_test_all[i])\n",
    "    print(image.format)\n",
    "    print(image.mode)\n",
    "    print(image.size)\n",
    "    #predict_chart(image)\n",
    "    img_array = keras.preprocessing.image.img_to_array(image)\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "    predictions = model.predict(img_array)\n",
    "    score = predictions[0]\n",
    "    print(\n",
    "    \"This image is %.2f percent Buy and %.2f percent Sell.\"\n",
    "    % (100 * (1 - score), 100 * score)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candlesticks_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepenv] *",
   "language": "python",
   "name": "conda-env-deepenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
